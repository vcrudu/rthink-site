[
  {
    "id": 1,
    "iconName": "squares_circles",
    "title": "Leveraging Retrieval to Improve Language Modeling Without Training",
    "text": "<p>Language models like GPT-3 have shown remarkable ability to generate fluent text, but they still struggle with factual accuracy since they lack grounding in external knowledge. This makes their outputs prone to hallucinations and factual errors. Retrieval augmented language modeling (RALM) has emerged as a promising approach to improve language models by retrieving relevant documents from a corpus to provide contextual grounding. However, most prior RALM methods require extensively modifying and retraining the language model architecture itself, which hinders wide adoption.</p><p>This post summarizes a recent paper \"In-Context Retrieval-Augmented Language Models\" (https://arxiv.org/abs/2302.00083) that proposes a simple yet highly effective RALM method called In-Context RALM. It can substantially improve off-the-shelf frozen language models without any architectural changes or retraining.</p><p>The key idea behind In-Context RALM is straightforward: during inference, retrieve a relevant document conditioned on the input text prefix, and prepend it to the language model's input. This allows the model to leverage external grounding knowledge when making predictions, while keeping the model weights frozen.</p><p>The authors first demonstrate that simply using an off-the-shelf sparse lexical retriever like BM25 already provides significant perplexity reductions of 2-3x across diverse language modeling datasets. For example, a 345M GPT-2 enhanced by In-Context RALM outperforms a 762M GPT-2, despite no training.</p><p>But they go further and show additional gains by tailoring document ranking specifically to the language modeling task:</p><p>Zero-shot reranking: Use a small LM to rerank retrieved documents based on likelihood of the upcoming text.</p><p>Predictive reranking: Train a dedicated bidirectional reranker using self-supervision from the LM's predictions.</p><p>With predictive reranking, a 345M GPT-2 enhanced by In-Context RALM actually outperforms a much larger 1.5B GPT-2. This demonstrates the power of specialized ranking functions.</p><p>The authors also validate In-Context RALM on open-domain QA, where it improved Exact Match on Natural Questions by 18 points. Overall, the paper shows how retrieval can significantly enhance language models without any architectural changes or retraining.</p><p>Conclusion:</p><p>In-Context RALM provides a simple yet highly effective approach to improve language models through retrieval augmentation, while keeping the model frozen. This opens opportunities to deploy more accurate grounded LMs in applications by just prepending relevant documents at inference time. As retriever models continue to advance, techniques like In-Context RALM can help unlock further progress in language modeling and its applications.</p>"
  },
  {
    "id": 2,
    "iconName": "rectangles",
    "title": "Teaching Language Models to Follow Instructions with GPT-4",
    "text": "<p>A new study explores an intriguing method for advancing open-source language models: having them learn from instructional data generated by powerful proprietary models like GPT-4. Researchers generated over 50,000 English and Chinese examples of GPT-4 following natural language instructions. They released this dataset to help the community improve models like LLaMA.</p><p>The researchers then trained LLaMA models on the GPT-4 instructional data. In-depth human evaluations assessed the modelsâ€™ abilities to follow new instructions helpfully, honestly, and harmlessly. The LLaMA model tuned on GPT-4 data significantly outperformed the prior model tuned on GPT-3 data in helpfulness. It also achieved near parity with original GPT-4 responses.</p><p>To enable automatic evaluation, the researchers had GPT-4 provide numerical ratings and comparisons of different models' responses. Using this unique feedback, they trained a reward model to score new responses. In tests, the GPT-4 tuned LLaMA outperformed other models in the automatic ratings. However, proprietary models like GPT-4 still achieved higher scores.</p><p>Analyzing the instructional data exposes differences between GPT-3 and GPT-4. GPT-4 uses more diverse verbs and nouns, and generates longer, more nuanced responses. This higher response quality from GPT-4 likely explains why tuning on its data improves alignment and task completion abilities.</p><p>The study demonstrates that generating training data from powerful models can effectively transfer capabilities to smaller open-source models. With more GPT-4 data and larger model sizes, open models may continue gaining ground on proprietary counterparts.</p><p>Background on Language Models and Instruction Tuning</p><p>Recent years have seen rapid progress in natural language processing driven by advances in deep learning and computational power. Foundation language models like GPT-3 and GPT-4 are trained on massive amounts of text data to generate fluent language. Researchers have been exploring how to align these large models with human goals and values.</p><p>One approach is instruction tuning, where models are trained to follow natural language instructions for completing tasks. Models can be tuned on human-written instructions, or on auto-generated data. The new study explores using data from GPT-4, one of the most powerful language models created by Anthropic.</p><p>Generating high-quality instructional data is challenging. The phrasing and scope impact how well models generalize. Datasets like SuperGLUE focus on narrow academic tasks. But real-world instructions are more open-ended. Prior work by Wang et al. collected instructions for user-oriented applications.</p><p>So having advanced models like GPT-4 generate responses offers a promising path for instruction tuning. The researchers aimed to assess whether GPT-4's capabilities could transfer to smaller, open-source models through its instructional data. Their experiments and analysis provide insights into the approach.</p><p>Instructional Data Collection and Model Tuning</p><p>The researchers generated a dataset of over 50,000 English and Chinese examples using GPT-4. Each instance has an instruction, optional context, and a GPT-4 generated response. They tuned 7 billion parameter LLaMA models on this data, including one for each language.</p><p>In addition, they had GPT-4 provide quality ratings and comparisons between its own responses and other models. This unique feedback enabled training a reward model for automatic evaluation.</p><p>The instructional data, tuned models, and reward model code were all publicly released to benefit the community. The datasets can support further research into aligning language models through instruction tuning.</p><p>Human Evaluation of Model Alignment</p><p>A key component of the study was in-depth human evaluation of whether instruction tuning with GPT-4 data improved model alignment. They assessed three key criteria:</p><p>- Helpfulness - does the model assist with goals</p><p>- Honesty - does it provide accurate information </p><p>- Harmlessness - does it avoid harmful outputs</p><p>These measures indicate how well a model captures human values. Participants rated GPT-4 tuned LLaMA and other models on their responses to over 250 everyday instructions.</p><p>The LLaMA model tuned on GPT-4 data significantly outperformed the GPT-3 tuned version in helpfulness. It also achieved close parity with unedited GPT-4 on all three alignment criteria. This demonstrates the effectiveness of instruction tuning with GPT-4 generated data.</p><p>Analysis of Model Outputs</p><p>The researchers analyzed differences between the GPT-4 and GPT-3 instructional datasets. GPT-4 used more diverse verbs and nouns compared to GPT-3. It also produced longer, more nuanced responses.</p><p>This indicates the higher quality data from GPT-4 better encompasses variability in human instructions. The richness of the GPT-4 data is likely crucial for transferring stronger generalization abilities.</p><p>They also evaluated models on the challenging Unnatural Instructions benchmark. GPT-4 tuned LLaMA performed closer to GPT-4 than other models when instructions involved complex reasoning. This further suggests GPT-4 data can better instill open-ended reasoning abilities.</p><p>Automated Evaluation with Reward Model</p><p>In addition to human assessment, the researchers evaluated models using GPT-4's own numerical ratings and comparisons. They tested the tuned LLaMA models against others like GPT-3 tuned LLaMA and non-tuned LLaMA.</p><p>On unseen instructions, the GPT-4 tuned LLaMA again achieved the highest automated scores. The reward model evaluation corroborated the human evaluations, showing the benefits of instruction tuning with GPT-4 data.</p><p>However, on open-ended instructions, proprietary models still outperform open counterparts. This indicates room for improvement from additional data and model scaling.</p><p>Implications for Advancing Open Models</p><p>This study provides valuable evidence that generating instructional data from advanced models like GPT-4 can effectively transfer capabilities to smaller open-source models. Tuning on GPT-4 data enhanced helpfulness, honesty, reasoning ability, and other measures.</p><p>The public dataset and analyses offer useful resources for researchers to build on this direction. With more data and larger model sizes, it may be possible to close the gap between open and proprietary models.</p><p>The work also highlights important considerations around transparency and ethics. While the GPT-4 data improves some measures of alignment, it may perpetuate that model's weaknesses too. Careful evaluation and mitigation of potential harms will remain critical.</p><p>Overall, the approach demonstrates a promising path forward for democratizing AI. With rigorous oversight and collaboration between researchers across organizations, we may continue progress toward widely accessible assistants that safely benefit all of humanity.</p>"
  },
  {
    "id": 3,
    "iconName": "dots",
    "title": "Language Models Step-by-Step Towards Better Problem Solving",
    "text": "<p>Recent advances in large language models (LLMs) like GPT-3 and GPT-4 have shown that they can be quite capable at certain reasoning and problem solving tasks, especially short logical deductions. However, their performance noticeably drops on more complex multi-step reasoning problems that require longer-term planning and exploration of multiple solution possibilities. To address this limitation, researchers from Theta Labs have proposed a new framework called the Tree-of-Thought (ToL). The key insight is that rather than rely solely on the LLM, augment it with additional components including a prompter, checker, memory module and a controller. Together, they guide the LLM through a tree-based search process, allowing it to methodically explore different solution paths step-by-step.</p> <p>The prompter interacts with the LLM using natural language, encouraging it to generate intermediate solutions rather than directly attempt the full solution. The checker then validates whether the intermediate solution is logically valid based on predefined rules or learned models. Valid solutions get stored in the memory module.</p><p>The controller oversees the process, determining whether to proceed down the current branch or backtrack based on the checker's feedback and search status. This allows recovery from mistakes and expands the search space beyond what the LLM alone can achieve. The overall approach reflects how human minds tackle difficult reasoning - through trial and error rather than a purely linear thought process. <p> </p>The researchers tested this ToL technique by building a Sudoku puzzle solver. Given an initial incomplete grid, the prompter provides it to the LLM and asks it to fill one or more cells following Sudoku rules. The checker verifies whether the moves are valid. The process repeats, with the controller determining when to explore other paths if stuck. Experiments on 9x9, 16x16 and 25x25 puzzles showed the ToL-enhanced solver massively outperformed regular LLM-based solvers that rely on fewer shots and less systematic search. While rule-based components were used for Sudoku, the approach could extend to other domains like theorem proving by training neural checkers. Self-play between puzzle generator and solver modules could also further improve performance. </p><p>By effectively combining the broad knowledge and pattern recognition capabilities of LLMs with additional modules for search, memory and validation, the Tree-of-Thought technique demonstrates how we can achieve more robust reasoning. The modular approach also allows integrating different methods like neural networks and rules-based systems. </p><p>As LLMs continue rapidly advancing, augmenting them with supplementary capabilities tailored for specific use cases looks to be a promising direction. The researchers plan to explore more advanced training techniques like multi-agent reinforcement learning to further optimize and generalize the approach across more complex domains. </p><p>Rather than hitting limits on standalone large models, combining them with other techniques in a hybrid system offers an intriguing path towards increasingly capable and trustworthy AI. Just like how human minds tap into various faculties in tandem to reason and problem solve, such complementary orchestration of models, data and algorithms may prove key to unlocking stronger artificial intelligence.</p>"
  }
]
